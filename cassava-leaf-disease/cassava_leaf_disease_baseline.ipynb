{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import model_selection\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.io \n",
    "from PIL import Image\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\"    \n",
    "\n",
    "class ImgStats:\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    CASSAVA_MEAN = [0.4303, 0.4967, 0.3135]\n",
    "    CASSAVA_STD = [0.2203, 0.2232, 0.2114]\n",
    "\n",
    "# CONSTANTS\n",
    "class Config:\n",
    "    NUM_CLASSES = 5\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_FOLDS = 5\n",
    "    UNFREEZE_EPOCH_NO = 1\n",
    "    NUM_EPOCHS = 2\n",
    "    NUM_WORKERS = 8\n",
    "    INPUT_IMAGE_SIZE = (224,224)\n",
    "    IMG_MEAN = ImgStats.IMAGENET_MEAN\n",
    "    IMG_STD = ImgStats.IMAGENET_STD\n",
    "    FAST_DEV_RUN = False\n",
    "    PRECISION = 16\n",
    "    IMG_ROOT_FOLDER = \"./data/\"\n",
    "    PATIENCE = 5\n",
    "    SUBSET_ROWS_FRAC = 0.1\n",
    "    TRAIN_ON_SUBSET = True\n",
    "    RANDOM_SEED = 42\n",
    "    MODEL_TO_USE = Models.RESNEXT50\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.00036\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Config.IMG_ROOT_FOLDER + \"label_num_to_disease_map.json\") as label_mapping_file:\n",
    "    label_map = json.load(label_mapping_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_img_names = pd.read_csv(Config.IMG_ROOT_FOLDER + \"train.csv\")\n",
    "# Add a column with disease name\n",
    "df_train_img_names[\"disease\"] = df_train_img_names.apply(\n",
    "    lambda row: label_map[str(row[\"label\"])], axis=1\n",
    "    )\n",
    "df_train_img_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_img_names.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the disease distribution within the training samples. Distribution doesn't seem balanced with CMD outnumbering other disease types by a wide margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disease distribution\n",
    "disease_counts = df_train_img_names.disease.value_counts()\n",
    "print(disease_counts)\n",
    "plt.bar(disease_counts.index, disease_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Disease count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.TRAIN_ON_SUBSET:\n",
    "    df_train_img_names = df_train_img_names.sample(\n",
    "        frac=Config.SUBSET_ROWS_FRAC, \n",
    "        random_state=Config.RANDOM_SEED\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold    \n",
    "    return df     \n",
    "\n",
    "df_train_img_names = strat_kfold_dataframe(df_train_img_names, target_col_name=\"label\")\n",
    "df_train_img_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/03/08/image-mean-std.html\n",
    "# https://www.thoughtco.com/sum-of-squares-formula-shortcut-3126266\n",
    "def get_imgs_mean_stddev(dl_img, axis=None):    \n",
    "    \"\"\"Get the mean and standard deviation for images in a dataset / mini-batch.\n",
    "    img batch is of shape BS * C * H * W \n",
    "    where BS = batch_size or no of training samples \n",
    "    C = 3 ( RGB channels ), H = height of image matrix, W = width of image matrix\n",
    "    Args:\n",
    "        dl_imgs ([DataLoader]): image data loader\n",
    "        axis ([tuple of ints], optional): Axis along which mean and std dev is to be calculated.\n",
    "        Defaults to None.\n",
    "    Returns:\n",
    "        [tuple]: tuple of tensors with mean and std.dev. of the imgs\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    # sum of pixel values along RGB channels\n",
    "    psum = torch.Tensor([0.0, 0.0, 0.0])\n",
    "    # sum of squares of pixel values along RGB channels\n",
    "    psum_sq = torch.Tensor([0.0, 0.0, 0.0])        \n",
    "    num_img = 0    \n",
    "    img_h, img_w = 0, 0    \n",
    "    count = 0\n",
    "    for img, label in tqdm.tqdm(dl_img): \n",
    "        if count == 0:            \n",
    "            img_h = img.shape[2]       \n",
    "            img_w = img.shape[3]\n",
    "        num_img += img.shape[0]            \n",
    "        psum += img.sum(axis=[0, 2, 3])        \n",
    "        img_sq = img.square()\n",
    "        psum_sq += img_sq.sum(axis=[0, 2, 3])\n",
    "        count += 1\n",
    "    # pixel count of single img (index 1 is the height and index 2 is width of img)\n",
    "    img_pixel_count = img_h * img_w      \n",
    "    total_pixel_count = num_img * img_pixel_count   \n",
    "    # mean of pixel values across the dataset        \n",
    "    total_mean = psum / total_pixel_count    \n",
    "    # variance of pixel values across the dataset\n",
    "    total_var = (psum_sq / total_pixel_count) - (total_mean.square())    \n",
    "    total_std = torch.sqrt(total_var)\n",
    "    return total_mean, total_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a custom pytorch Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset contains the logic to fetch, load and if required transform data to bring it to a format\n",
    "# that can be used by dataloaders for training\n",
    "class CassavaImageDataset(Dataset):\n",
    "    def __init__(self, df, img_name_col, target_col, img_root_folder, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.img_name_col = img_name_col\n",
    "        self.target_col = target_col\n",
    "        self.img_root_folder = img_root_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_root_folder + \"/\" + self.df.loc[index, self.img_name_col]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img_label = self.df.loc[index, self.target_col]\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented[\"image\"]\n",
    "        if self.target_transform is not None:\n",
    "            img_label = self.target_transform(img_label)\n",
    "        return img, img_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = alb.Compose([\n",
    "        alb.RandomResizedCrop(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]),        \n",
    "        alb.Transpose(p=0.5),                \n",
    "        alb.HorizontalFlip(p=0.5),\n",
    "        alb.VerticalFlip(p=0.5),\n",
    "        alb.ShiftScaleRotate(p=0.5),\n",
    "        alb.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5),        \n",
    "        alb.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5),\n",
    "        alb.Normalize(mean=Config.IMG_MEAN, std=Config.IMG_STD),\n",
    "        ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = alb.Compose([\n",
    "        alb.CenterCrop(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]),\n",
    "        alb.Normalize(mean=Config.IMG_MEAN, std=Config.IMG_STD),\n",
    "        ToTensorV2()        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df_imgs):\n",
    "    df_train = df_imgs[df_imgs[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    df_val = df_imgs[df_imgs[\"kfold\"] == fold].reset_index(drop=True)    \n",
    "    ds_train = CassavaImageDataset(\n",
    "        df_train, \n",
    "        img_name_col=\"image_id\",\n",
    "        target_col=\"label\",\n",
    "        img_root_folder=Config.IMG_ROOT_FOLDER + \"train_images\", \n",
    "        transform=train_transform,\n",
    "        target_transform=torch.as_tensor\n",
    "        )\n",
    "    ds_val = CassavaImageDataset(\n",
    "        df_val, \n",
    "        img_name_col=\"image_id\",\n",
    "        target_col=\"label\",\n",
    "        img_root_folder=Config.IMG_ROOT_FOLDER + \"train_images\", \n",
    "        transform=val_transform,\n",
    "        target_transform=torch.as_tensor\n",
    "        )        \n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)    \n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, df_train_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# for imgs, lbls in dl_val:\n",
    "#     if counter > 0:\n",
    "#         break\n",
    "#     img_mean = torch.mean(imgs, axis=(0, 2, 3))\n",
    "#     img_std = torch.std(imgs, axis=(0, 2, 3))\n",
    "#     counter += 1\n",
    "# print(img_mean, img_std)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_all = CassavaImageDataset(\n",
    "#         df_train_img_names, \n",
    "#         img_name_col=\"image_id\",\n",
    "#         target_col=\"label\",\n",
    "#         img_root_folder=\"./data/train_images\", \n",
    "#         transform=train_transform,\n",
    "#         target_transform=torch.as_tensor\n",
    "#         )\n",
    "# dl_all = DataLoader(ds_all, batch_size=BATCH_SIZE, shuffle=True)            \n",
    "# img_mean, img_std = get_imgs_mean_stddev(dl_all)\n",
    "# print(f\"img mean calculated on entire training dataset = {img_mean}\")\n",
    "# print(f\"img std calculated on entire training dataset = {img_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img mean calculated on entire training dataset = tensor([0.4303, 0.4967, 0.3135])<br>\n",
    "img std calculated on entire training dataset = tensor([0.2203, 0.2232, 0.2114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(Image.fromarray(np.uint8(img)).convert('RGB'))        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(ds_val, 3, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional\n",
    "# from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
    "\n",
    "# class CassavaDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, fold, img_root_path, df, img_name_col, target_col, batch_size=BATCH_SIZE,  \n",
    "#                  input_img_dims=(3,INPUT_IMAGE_SIZE[0],INPUT_IMAGE_SIZE[1]),num_target_classes=5, \n",
    "#                  train_transforms=None, val_transforms=None):  \n",
    "#         super().__init__()                       \n",
    "#         self.fold = fold\n",
    "#         self.img_root_path = img_root_path  \n",
    "#         self.df = df\n",
    "#         self.img_name_col = img_name_col\n",
    "#         self.target_col = target_col\n",
    "#         self.batch_size = batch_size\n",
    "#         self.input_imgs_dims = input_img_dims        \n",
    "#         self.num_target_classes = num_target_classes        \n",
    "#         if train_transforms is None:\n",
    "#             self.train_transforms = None\n",
    "#         else:\n",
    "#             self.train_transforms = train_transforms    \n",
    "\n",
    "#         if val_transforms is None:\n",
    "#             self.val_transforms = None\n",
    "#         else:\n",
    "#             self.val_transforms = val_transforms                        \n",
    "\n",
    "#     def prepare_data(self) -> None:\n",
    "#          return super().prepare_data()       \n",
    "\n",
    "#     def setup(self, stage: Optional[str] = None) -> None:\n",
    "#         super().setup(stage=stage)                     \n",
    "#         self.df_train = self.df[self.df[\"kfold\"] != self.fold].reset_index(drop=True)\n",
    "#         self.df_val = self.df[self.df[\"kfold\"] == self.fold].reset_index(drop=True)  \n",
    "#         self.ds_train = CassavaImageDataset(\n",
    "#             self.df_train, \n",
    "#             img_name_col=self.img_name_col,\n",
    "#             target_col=self.target_col,\n",
    "#             img_root_folder=self.img_root_path, \n",
    "#             transform=self.train_transforms,\n",
    "#             target_transform=torch.as_tensor\n",
    "#             )\n",
    "#         self.ds_val = CassavaImageDataset(\n",
    "#             self.df_val, \n",
    "#             img_name_col=self.img_name_col,\n",
    "#             target_col=self.target_col,\n",
    "#             img_root_folder=self.img_root_path, \n",
    "#             transform=self.val_transforms,\n",
    "#             target_transform=torch.as_tensor\n",
    "#             )            \n",
    "\n",
    "#     def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "#         return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True, \n",
    "#                             num_workers=NUM_WORKERS)            \n",
    "\n",
    "#     def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "#         return DataLoader(self.ds_val, batch_size=self.batch_size, num_workers=NUM_WORKERS)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_name_col=\"image_id\"\n",
    "# target_col=\"label\"\n",
    "# img_root_folder=\"./data/train_images\"\n",
    "\n",
    "# dm = CassavaDataModule(\n",
    "#     fold = 0, \n",
    "#     df = df_train_img_names,\n",
    "#     img_root_path = img_root_folder,\n",
    "#     img_name_col = img_name_col,\n",
    "#     target_col = target_col,\n",
    "#     train_transforms = train_transform,\n",
    "#     val_transforms = val_transform\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "import timm\n",
    "\n",
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, hparams, model_to_use):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.num_classes = num_classes        \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(model_to_use, hparams[\"drop_out\"], num_classes) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"val_acc\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].cpu().detach().item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}, val_loss={val_loss}\")        \n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "\n",
    "def run_training(fold, dl_train, dl_val, fold_loss, fold_acc, find_lr=True):\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        tb_logger = None\n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"        \n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "        #print_table_metric_cb = PrintTableMetricsCallback()\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", version=fold_str)\n",
    "        else:\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")        \n",
    "        cassava_model = ImageClassificationLitModel(\n",
    "            num_classes=Config.NUM_CLASSES, \n",
    "            hparams=Config.MODEL_PARAMS,        \n",
    "            model_to_use=Config.MODEL_TO_USE\n",
    "            )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            auto_select_gpus=True,\n",
    "            progress_bar_refresh_rate=20,\n",
    "            max_epochs=Config.NUM_EPOCHS,\n",
    "            logger=tb_logger,\n",
    "            auto_lr_find=True,    \n",
    "            precision=Config.PRECISION,    \n",
    "            weights_summary=None, \n",
    "            fast_dev_run=Config.FAST_DEV_RUN,                   \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        if find_lr:\n",
    "            trainer.tune(model=cassava_model, train_dataloaders=dl_train)\n",
    "            print(cassava_model.lr)\n",
    "        trainer.fit(cassava_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        if not Config.FAST_DEV_RUN:\n",
    "            fold_loss.append(loss_chkpt_callback.best_model_score.cpu().detach().item())\n",
    "            fold_acc.append(acc_chkpt_callback.best_metric)\n",
    "            print(f\"Loss for {fold_str} = {fold_loss[fold]}, accuracy = {fold_acc[fold]}\")\n",
    "        del trainer, cassava_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def print_exp_statistics(fold_loss, fold_acc):\n",
    "    print(\"Loss across folds\")\n",
    "    print(fold_loss)\n",
    "    print(\"Accuracy across folds\")\n",
    "    print(fold_acc)\n",
    "    #mean_loss = statistics.mean(fold_loss)\n",
    "    #mean_acc = statistics.mean(fold_acc)\n",
    "    #std_loss = statistics.stdev(fold_loss)\n",
    "    #std_acc = statistics.stdev(fold_acc)\n",
    "    #print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "    #print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    find_lr = True\n",
    "    fold_loss = []\n",
    "    fold_acc = []\n",
    "    for fold in range(Config.NUM_FOLDS):\n",
    "        dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, df_train_img_names)\n",
    "        run_training(fold, dl_train, dl_val, fold_loss, fold_acc, find_lr)\n",
    "        break  \n",
    "    print_exp_statistics(fold_loss, fold_acc)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for experiment 1\n",
    "\n",
    "Config.MODEL_TO_USE = Models.RESNET50\n",
    "Config.NUM_EPOCHS = 10\n",
    "Config.IMG_MEAN = ImgStats.IMAGENET_MEAN\n",
    "Config.IMG_STD = ImgStats.IMAGENET_STD\n",
    "Config.INPUT_IMAGE_SIZE = (512,512)\n",
    "Config.BATCH_SIZE = 64\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.MODEL_TO_USE = Models.RESNEXT50\n",
    "Config.NUM_EPOCHS = 10\n",
    "Config.IMG_MEAN = ImgStats.IMAGENET_MEAN\n",
    "Config.IMG_STD = ImgStats.IMAGENET_STD\n",
    "Config.INPUT_IMAGE_SIZE = (512,512)\n",
    "Config.BATCH_SIZE = 48\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResnetTL_LitModel.load_from_checkpoint(\"./model/cassava_best_model.ckpt\",     \n",
    "#                                                 num_classes=5)\n",
    "# model.to(\"cuda\")\n",
    "# model.eval()\n",
    "\n",
    "# incorrect = 0\n",
    "# total = 0\n",
    "# predicted_labels_incorrect = []\n",
    "# labels_incorrect = []\n",
    "# with torch.no_grad():\n",
    "#     counter=0\n",
    "#     for imgs, labels in tqdm.tqdm(dm.val_dataloader()):                \n",
    "#         predicted_cuda_labels = torch.argmax(model(imgs.to(\"cuda\")), dim=1)\n",
    "#         predicted_labels = predicted_cuda_labels.cpu().detach()\n",
    "#         total += labels.shape[0]\n",
    "#         correct_pred = predicted_labels == labels\n",
    "#         incorrect_pred = ~correct_pred\n",
    "#         num_incorrect_pred = incorrect_pred.sum()\n",
    "#         incorrect += int(num_incorrect_pred)\n",
    "#         if num_incorrect_pred > 0:\n",
    "#             predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "#             labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "# print(f'Total no. of images in validation set: {total}')\n",
    "# print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "# accuracy = ((total-incorrect) / total) * 100        \n",
    "# print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_finder = trainer.tuner.lr_find(cassava_model, dm)\n",
    "# print(lr_finder.suggestion())\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
